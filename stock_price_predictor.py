# -*- coding: utf-8 -*-
"""Stock_Price_Predictor.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1eLA5PlL6SbStcizYFT98AqWg98NydeaM

# Machine Learning Engineer Nanodegree
## Capstone Project
## Project: Stock Price Predictor

The challenge of this project is to accurately predict the future closing value of a given stock across a given period of time in the future. For this project I will use a [Long Short Term Memory networks – usually just called “LSTMs”](http://colah.github.io/posts/2015-08-Understanding-LSTMs/) to predict the closing price of the [S&P 500](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies) using a dataset of past prices

## Get the Data

In the following cells we download and save the [S&P 500 dataset](https://en.wikipedia.org/wiki/List_of_S%26P_500_companies).

   **Step 1 :** Define a function to get historical data from google finance
"""

import yfinance as yf

def get_historical_data(symbol, start_date, end_date):
    ''' Daily quotes from Yahoo Finance. Date format='yyyy-mm-dd' '''
    symbol = symbol.upper()
    # Download historical data using Yahoo Finance
    data = yf.download(symbol, start=start_date, end=end_date)

    # Rename columns to match your previous naming convention
    data = data[['Open', 'High', 'Low', 'Close', 'Volume']]
    data['Date'] = data.index
    data = data[['Date', 'Open', 'High', 'Low', 'Close', 'Volume']]

    return data

# Example usage:
data = get_historical_data('GOOGL', '2021-11-01', '2024-06-30')
print(data.head())

""" **Step 2:** get the data of desired firm from [Google Finance](http://www.google.com/finance)."""

data = get_historical_data('GOOGL','2021-11-01','2024-06-30') # from January 1, 2005 to June 30, 2017

"""**Step 3:** Write the data to a csv file."""

data.to_csv('google.csv',index = False)

"""# Check Point #1

This is my first checkpoint. The data has been saved to disk.

## Preprocess the data

Now it is time to preprocess the data. In the following cells we will normalise it for better prediction of data.

**Step 1 :** Get the data from csv file.
"""

import pandas as pd
import numpy as np

# Step 1: Load the data from CSV, and check the column names first
data = pd.read_csv('google.csv', skiprows=1)

# Print the column names to inspect
print("Columns in the dataset:", data.columns)

# Step 2: If the column names aren't correct, manually rename them
data.columns = ['Date', 'Open', 'High', 'Low', 'Close', 'Volume']

# Step 3: Convert columns to numeric, forcing errors to NaN
data['Open'] = pd.to_numeric(data['Open'], errors='coerce')
data['High'] = pd.to_numeric(data['High'], errors='coerce')
data['Low'] = pd.to_numeric(data['Low'], errors='coerce')
data['Close'] = pd.to_numeric(data['Close'], errors='coerce')
data['Volume'] = pd.to_numeric(data['Volume'], errors='coerce')

# Step 4: Print the first few rows to ensure data looks correct
print(data.head())

# Step 5: Now calculate and print the statistics for each column
print("\n")
print("Open   --- mean :", np.mean(data['Open']),  "  \t Std: ", np.std(data['Open']),  "  \t Max: ", np.max(data['Open']),  "  \t Min: ", np.min(data['Open']))
print("High   --- mean :", np.mean(data['High']),  "  \t Std: ", np.std(data['High']),  "  \t Max: ", np.max(data['High']),  "  \t Min: ", np.min(data['High']))
print("Low    --- mean :", np.mean(data['Low']),   "  \t Std: ", np.std(data['Low']),   "  \t Max: ", np.max(data['Low']),   "  \t Min: ", np.min(data['Low']))
print("Close  --- mean :", np.mean(data['Close']), "  \t Std: ", np.std(data['Close']), "  \t Max: ", np.max(data['Close']), "  \t Min: ", np.min(data['Close']))
print("Volume --- mean :", np.mean(data['Volume']),"  \t Std: ", np.std(data['Volume']),"  \t Max: ", np.max(data['Volume']),"  \t Min: ", np.min(data['Volume']))

"""**Step 2 :** Remove Unncessary data, i.e., Date and High value"""

import preprocess_data as ppd
stocks = ppd.remove_data(data)

#Print the dataframe head and tail
print(stocks.head())
print("---")
print(stocks.tail())

"""**Step 2: ** Visualise raw data."""

import visualize

visualize.plot_basic(stocks)

"""**Step 3 :** Normalise the data using minmaxscaler function"""

stocks = ppd.get_normalised_data(stocks)
print(stocks.head())

print("\n")
print("Open   --- mean :", np.mean(stocks['Open']),  "  \t Std: ", np.std(stocks['Open']),  "  \t Max: ", np.max(stocks['Open']),  "  \t Min: ", np.min(stocks['Open']))
print("Close  --- mean :", np.mean(stocks['Close']), "  \t Std: ", np.std(stocks['Close']), "  \t Max: ", np.max(stocks['Close']), "  \t Min: ", np.min(stocks['Close']))
print("Volume --- mean :", np.mean(stocks['Volume']),"  \t Std: ", np.std(stocks['Volume']),"  \t Max: ", np.max(stocks['Volume']),"  \t Min: ", np.min(stocks['Volume']))

"""**Step 4 :** Visualize the data again"""

visualize.plot_basic(stocks)

"""**Step 5:** Log the normalised data for future resuablilty"""

stocks.to_csv('google_preprocessed.csv',index= False)

"""# Check Point #2

This is my second checkpoint. The preprocessed data has been saved to disk.

## Bench Mark Model

In this section we will check our bench mark model. As is proposed in my proposal my bench mark model is a simple linear regressor model.

**Step 1:** Load the preprocessed data
"""

import math
import pandas as pd
import numpy as np
from IPython.display import display
from sklearn import linear_model
from sklearn.model_selection import train_test_split
from sklearn.metrics import mean_squared_error
from sklearn.model_selection import TimeSeriesSplit

import visualize as vs
import stock_data as sd
import LinearRegressionModel

stocks = pd.read_csv('google_preprocessed.csv')
display(stocks.head())

"""**Step 2:** Split data into train and test pair"""

X_train, X_test, y_train, y_test, label_range= sd.train_test_split_linear_regression(stocks)

print("x_train", X_train.shape)
print("y_train", y_train.shape)
print("x_test", X_test.shape)
print("y_test", y_test.shape)

"""**Step 3:** Train a Linear regressor model on training set and get prediction"""

model = LinearRegressionModel.build_model(X_train,y_train)

"""**Step 4:** Get prediction on test set"""

predictions = LinearRegressionModel.predict_prices(model,X_test, label_range)

"""**Step 5:** Plot the predicted values against actual"""

vs.plot_prediction(y_test,predictions)

"""**Step 6:** measure accuracy of the prediction"""

trainScore = mean_squared_error(X_train, y_train)
print('Train Score: %.4f MSE (%.4f RMSE)' % (trainScore, math.sqrt(trainScore)))

testScore = mean_squared_error(predictions, y_test)
print('Test Score: %.8f MSE (%.8f RMSE)' % (testScore, math.sqrt(testScore)))

"""# Checkpoint #3


## Long-Sort Term Memory Model

In this section we will use LSTM to train and test on our data set.

### Basic LSTM Model

First lets make a basic LSTM model.

**Step 1 :** import keras libraries for smooth implementaion of lstm
"""

import math
import pandas as pd
import numpy as np
from IPython.display import display

# Import from tensorflow.keras instead of keras
from tensorflow.keras.layers import Dense, Activation, Dropout
from tensorflow.keras.layers import LSTM
from tensorflow.keras.models import Sequential
# Import mean_squared_error from tensorflow.keras.losses instead of tensorflow.keras.metrics
!pip install pyod
import tensorflow as tf
from pyod.models.auto_encoder import AutoEncoder
from tensorflow.keras.losses import MeanSquaredError
# Make sure to install tensorflow if not already installed
!pip install tensorflow

import lstm, time #helper libraries

import visualize as vs
import stock_data as sd
import LinearRegressionModel

stocks = pd.read_csv('google_preprocessed.csv')
stocks_data = stocks.drop(['Item'], axis =1)

display(stocks_data.head())

# Load data and check column names
stocks = pd.read_csv('google_preprocessed.csv')
print("Columns in dataset:", stocks.columns)
print(stocks.head())

print(stocks.isnull().sum())  # Display count of missing values for each column

def train_test_split_lstm(stocks, prediction_time=1, test_data_size=450, unroll_length=50):
    """
    Splits the dataset into training and testing features for Long Short Term Memory Model.
    """
    # Calculate total data length
    data_len = len(stocks)

    # Calculate starting indices for test data
    test_start_x = data_len - test_data_size - unroll_length - prediction_time
    test_start_y = test_start_x + prediction_time

    # Extract training data
    x_train = stocks.iloc[0:test_start_x].to_numpy()  # Ensure data is in array format
    y_train = stocks['Close'].iloc[prediction_time:test_start_x + prediction_time].to_numpy()

    # Extract testing data
    x_test = stocks.iloc[test_start_x:data_len - prediction_time].to_numpy()
    y_test = stocks['Close'].iloc[test_start_y:data_len].to_numpy()

    return x_train, x_test, y_train, y_test

X_train, X_test, y_train, y_test = train_test_split_lstm(stocks, prediction_time=5)
print("Train/Test shapes:", X_train.shape, y_train.shape, X_test.shape, y_test.shape)

"""**Step 2 :** Split train and test data sets and Unroll train and test data for lstm model"""

X_train, X_test,y_train, y_test = train_test_split_lstm(stocks_data, 5)

unroll_length = 50
X_train = sd.unroll(X_train, unroll_length)
X_test = sd.unroll(X_test, unroll_length)
y_train = y_train[-X_train.shape[0]:]
y_test = y_test[-X_test.shape[0]:]

print("x_train", X_train.shape)
print("y_train", y_train.shape)
print("x_test", X_test.shape)
print("y_test", y_test.shape)

"""**Step 3 :** Build a basic Long-Short Term Memory model"""

# build basic lstm model
model = lstm.build_basic_model(input_dim = X_train.shape[-1],output_dim = unroll_length, return_sequences=True)

# Compile the model
start = time.time()
model.compile(loss='mean_squared_error', optimizer='adam')
print('compilation time : ', time.time() - start)

"""**Step 4:** Train the model"""

model.fit(
    X_train,
    y_train,
    epochs=1,
    validation_split=0.05)

"""**Step 5:** make prediction using test data"""

predictions = model.predict(X_test)

"""**Step 6:** Plot the results"""

vs.plot_lstm_prediction(y_test,predictions)

"""** Step 7:** Get the test score."""

trainScore = model.evaluate(X_train, y_train, verbose=0)
print('Train Score: %.8f MSE (%.8f RMSE)' % (trainScore, math.sqrt(trainScore)))

testScore = model.evaluate(X_test, y_test, verbose=0)
print('Test Score: %.8f MSE (%.8f RMSE)' % (testScore, math.sqrt(testScore)))

"""### Improved LSTM Model

**Step 1: ** Build an improved LSTM model
"""

# Set up hyperparameters
batch_size = 100
epochs = 5

# build improved lstm model
model = lstm.build_improved_model( X_train.shape[-1],output_dim = unroll_length, return_sequences=True)

start = time.time()
#final_model.compile(loss='mean_squared_error', optimizer='adam')
model.compile(loss='mean_squared_error', optimizer='adam')
print('compilation time : ', time.time() - start)

"""**Step 2: ** Train improved LSTM model"""

model.fit(X_train,
          y_train,
          batch_size=batch_size,
          epochs=epochs,
          verbose=2,
          validation_split=0.05
         )

"""**Step 3:** Make prediction on improved LSTM model"""

# Generate predictions
predictions = model.predict(X_test, batch_size=batch_size)

"""**Step 4:** plot the results"""

vs.plot_lstm_prediction(y_test,predictions)

"""**Step 5:** Get the test score"""

trainScore = model.evaluate(X_train, y_train, verbose=0)
print('Train Score: %.8f MSE (%.8f RMSE)' % (trainScore, math.sqrt(trainScore)))

testScore = model.evaluate(X_test, y_test, verbose=0)
print('Test Score: %.8f MSE (%.8f RMSE)' % (testScore, math.sqrt(testScore)))

range = [np.amin(stocks_data['Close']), np.amax(stocks_data['Close'])]

#Calculate the stock price delta in $

true_delta = testScore*(range[1]-range[0])
print('Delta Price: %.6f - RMSE * Adjusted Close Range' % true_delta)

"""# Checking Robustness of the model

In this section we will check robustness of our LSTM model. I have used new unseen datasets for this from July 1, 2017 to July 20,2017. I have downloaded the data sets from google finance website to check for robustness of the model.
"""

import preprocess_data as ppd

data = pd.read_csv('googl.csv')

stocks = ppd.remove_data(data)

stocks = ppd.get_normalised_data(stocks)

stocks = stocks.drop(['Item'], axis = 1)
#Print the dataframe head and tail
print(stocks.head())

X = stocks[:].to_numpy()
Y = stocks[:]['Close'].to_numpy()

X = sd.unroll(X,1)
Y = Y[-X.shape[0]:]

print(X.shape)
print(Y.shape)

# Generate predictions
predictions = model.predict(X)

#get the test score
testScore = model.evaluate(X, Y, verbose=0)
print('Test Score: %.4f MSE (%.4f RMSE)' % (testScore, math.sqrt(testScore)))

